\documentclass{article}

% Pre-load amsmath and set natbib options
\usepackage{amsmath}
\PassOptionsToPackage{numbers, compress}{natbib}

% Load NeurIPS style
\usepackage[preprint]{neurips_2025}

% Core packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{xcolor}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{float}
\usepackage{algorithm}
\usepackage{algorithmic}

% Setup hyperref for citation jumping
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    citecolor=blue,
    linkcolor=red,
    urlcolor=magenta,
}

\title{Convex Optimization Project: Robust Prompt Learning via Optimal Transport and Adaptive GCE}

\author{%
  Zihan Wang, Zhichen Zhong, Yixuan Liu, Haoyu Li \\
  School of Information Science and Technology\\
  ShanghaiTech University \\
  \texttt{\{wangzh12023, zhongzhch2023, liuyx2023, lihy2023\}@shanghaitech.edu.cn} \\
}

\begin{document}

\maketitle

\begin{abstract}
    Vision-Language Models (VLMs) like CLIP have revolutionized representation learning. However, prompt learning methods designed for these models remain vulnerable to noisy labels in downstream tasks. This project focuses on the reproduction and extension of \textit{NLPrompt} \citep{pan2024nlprompt}, a framework that utilizes Optimal Transport (OT) to purify noisy data. From a convex optimization perspective, we provide a rigorous analysis of the OT formulation via Bregman Projections and address the challenge of class imbalance by mathematically deriving the correct marginal constraints that define the transport polytope. Furthermore, identifying that the original "hard partition" strategy can misclassify clean samples as noisy (particularly in low-noise regimes), we propose a novel improvement: \textbf{OT-Guided Adaptive Generalized Cross Entropy (OT-AdaGCE)}. Instead of a binary separation that forces potentially clean data into an inefficient MAE loss, our method dynamically adjusts the robustness parameter $q$ based on OT confidence scores. This allows for a smooth transition between noise-robustness and learning efficiency. Our experimental results on benchmarks including Flowers102, DTD, and EuroSAT demonstrate the efficacy of the reproduction and the potential of the proposed soft-weighting mechanism.
\end{abstract}
\section{Introduction}

The advent of Vision-Language Models (VLMs) such as CLIP has bridged the gap between visual and textual data. Prompt learning has emerged as a parameter-efficient fine-tuning method for these models \cite{pan2024nlprompt}. However, real-world datasets are inherently noisy, and standard Cross-Entropy (CE) loss is known to overfit to incorrect labels, leading to significant performance degradation.

The paper \textit{NLPrompt: Noise-Label Prompt Learning for Vision-Language Models} \cite{pan2024nlprompt} addresses this challenge by employing Optimal Transport (OT) to align image features with text prototypes globally, thereby identifying and "purifying" noisy labels. The original method partitions data into "clean" and "noisy" subsets based on OT results, applying CE loss to the former and a robust Mean Absolute Error (MAE) loss to the latter \cite{zhang2018gce}. While effective, this binary "hard partition" strategy is heuristic and discards the fine-grained confidence information contained in the optimal transport plan. Furthermore, the original formulation lacks a rigorous derivation regarding how to correctly handle class imbalance.

In this project, we aim to:
\begin{enumerate}
    \item \textbf{Replicate} the NLPrompt framework and verify its performance on standard benchmarks.
    \item \textbf{Analyze} the theoretical underpinnings of the OT formulation from a convex optimization perspective. We explicitly formulate the entropic-regularized OT problem as a Bregman Projection of the Gibbs kernel onto the transport polytope, and mathematically derive the correct marginal constraints to incorporate class priors for handling class imbalance.
    \item \textbf{Propose} an incremental innovation: OT-Guided Adaptive Generalized Cross Entropy (OT-AdaGCE). We replace the heuristic hard thresholding with a smooth, instance-dependent weighting mechanism that dynamically adjusts the robustness parameter $q$ based on transport confidence, mitigating the information loss inherent in binary partitioning.
\end{enumerate}
% \section{Introduction}

% The advent of Vision-Language Models (VLMs) such as CLIP has bridged the gap between visual and textual data \cite{pan2024nlprompt}. Prompt learning has emerged as a parameter-efficient fine-tuning method for these models \cite{pan2024nlprompt}. However, real-world datasets are inherently noisy, and standard Cross-Entropy (CE) loss is known to overfit to incorrect labels, leading to performance degradation \cite{pan2024nlprompt}.

% The paper \textit{NLPrompt: Taming Noisy Labels in Vision-Language Models} \cite{pan2024nlprompt} addresses this by employing Optimal Transport (OT) to align image features with text prototypes, thereby identifying and "purifying" noisy labels. The original method partitions data into "clean" and "noisy" subsets, applying CE loss to the former and Mean Absolute Error (MAE) loss to the latter \cite{pan2024nlprompt, zhang2018gce}.

% In this project, we aim to:
% \begin{enumerate}
%     \item \textbf{Replicate} the NLPrompt framework and verify its performance on standard benchmarks.
%     \item \textbf{Analyze} the theoretical underpinnings of the OT formulation from a convex optimization perspective, specifically focusing on the transport polytope and entropic regularization.
%     \item \textbf{Propose} an incremental innovation: an OT-Guided Adaptive GCE loss that replaces the heuristic hard thresholding with a smooth, instance-dependent weighting mechanism.
% \end{enumerate}

\section{Preliminaries: The NLPrompt Framework}

\label{sec:preliminaries}
In this section, we briefly review the NLPrompt framework \cite{pan2024nlprompt}, which utilizes Optimal Transport (OT) to purify noisy labels. Given a batch of $N$ images and $K$ class text prototypes, NLPrompt first computes the similarity logits $S = \mathbf{T} \mathbf{I}^\top \in \mathbb{R}^{K \times N}$ between the normalized text features $\mathbf{T}$ and image features $\mathbf{I}$.

To construct the cost matrix $C$, the similarity logits are converted into probabilities via a column-wise Softmax operation (ensuring the probability distribution for each image sums to 1), followed by a negative logarithm:
\begin{equation}
    C = -\log(\text{Softmax}(\mathbf{T} \cdot \mathbf{I}^\top))
\end{equation}
Here, $C_{ki}$ represents the cost of assigning image $i$ to class $k$. To align images with prototypes globally, NLPrompt solves the following entropic-regularized OT problem:
\begin{equation}
    \label{eq:ot_problem}
    Q^* = \arg\min_{Q \in \mathbb{R}_{+}^{K \times N}} \langle C, Q \rangle - \epsilon H(Q) \quad \text{s.t.} \quad Q \mathbf{1}_N = \frac{1}{K}\mathbf{1}_K, \, Q^\top \mathbf{1}_K = \frac{1}{N}\mathbf{1}_N
\end{equation}
where $\langle \cdot, \cdot \rangle$ denotes the Frobenius dot product, and $H(Q)$ is the entropic regularization term. The constraints enforce specific marginal distributions: the sample marginal $Q^\top \mathbf{1}_K = \frac{1}{N}\mathbf{1}_N$ ensures every image is treated equally with weight $1/N$, while the class marginal $Q \mathbf{1}_N = \frac{1}{K}\mathbf{1}_K$ enforces an equipartition constraint (uniform prior) over the $K$ classes. The optimal transport plan $Q^*$ is then used to generate pseudo-labels for data partitioning. The complete purification and training process is summarized in Algorithm~\ref{alg:nlprompt}.

\begin{algorithm}[H]
    \caption{NLPrompt: (Re-implementation)}
    \label{alg:nlprompt}
    \begin{algorithmic}[1]
        \REQUIRE Image encoder $g$, Text encoder $h$, Class prompts $\mathbf{p}$, Batch $\{(x_i, y_i)\}_{i=1}^B$
        \STATE Compute image features $\mathbf{I} = g(x)$ and text features $\mathbf{T} = h(\mathbf{p})$
        \STATE Compute logits $S = \mathbf{T} \mathbf{I}^\top$ and Cost Matrix $C = -\log(\text{Softmax}(S))$
        \STATE \textbf{Solve OT:} Obtain $Q^*$ via Sinkhorn algorithm for Eq.~\eqref{eq:ot_problem}
        \STATE \textbf{Generate Pseudo-labels:} $\tilde{y}_i = \arg\max_{j} Q^*_{ji}$
        \FOR{each sample $(x_i, y_i)$}
            \IF{$\tilde{y}_i == y_i$}
                \STATE Update prompts using \textbf{CE Loss}: $\mathcal{L}_{CE}(f(x_i), y_i)$
            \ELSE
                \STATE Update prompts using \textbf{MAE Loss}: $\mathcal{L}_{MAE}(f(x_i), y_i)$
            \ENDIF
        \ENDFOR
        \RETURN Updated prompts
    \end{algorithmic}
\end{algorithm}

\section{Theoretical Analysis}

A core contribution of this project is the analysis of the data purification process through the lens of Convex Optimization. We demonstrate that the entropic-regularized formulation in NLPrompt is mathematically equivalent to a Bregman Projection, and we derive the correct strategy for handling class imbalance based on the properties of the transport polytope.

\paragraph{Bregman Projection Viewpoint.}
First, we analyze the objective function in Eq.~\eqref{eq:ot_problem}. Let $\mathcal{U}(\alpha, \beta) = \{ Q \in \mathbb{R}_{+}^{K \times N} \mid Q \mathbf{1}_N = \alpha, \, Q^\top \mathbf{1}_K = \beta \}$ denote the transport polytope. We define the \textit{Gibbs Kernel} $K$ associated with the cost matrix $C$ as $K_{ij} = \exp(-C_{ij}/\epsilon)$. 
By expanding the Kullback-Leibler (KL) divergence between a transport plan $Q$ and the kernel $K$, we observe:
\begin{equation}
    D_{KL}(Q \| K) = \sum_{i,j} Q_{ij} \log \frac{Q_{ij}}{e^{-C_{ij}/\epsilon}} - Q_{ij} + K_{ij} = \frac{1}{\epsilon} \langle C, Q \rangle - H(Q) + \text{const.}
\end{equation}
Consequently, minimizing the entropic OT objective is equivalent to minimizing the KL divergence (a specific Bregman divergence) between $Q$ and the Gibbs kernel $K$:
\begin{equation}
    Q^* = \arg\min_{Q \in \mathcal{U}(\alpha, \beta)} D_{KL}(Q \| K)
\end{equation}
Geometrically, this implies that the solution $Q^*$ is the \textbf{Bregman Projection} of the geometry-agnostic prior $K$ onto the feasible set $\mathcal{U}$. The Sinkhorn algorithm can thus be interpreted as a sequence of alternating Bregman projections onto the row and column constraints of the polytope.

\paragraph{Derivation for Class-Imbalanced Constraints.}
Based on the Bregman projection perspective established above, handling class imbalance naturally corresponds to projecting the Gibbs kernel $K$ onto a \textit{reshaped} transport polytope $\mathcal{U}(\pi, \frac{1}{N}\mathbf{1}_N)$, where $\pi$ is the estimated class prior.

A theoretical question arises: to incorporate this prior, should we also modify the reference measure (the Gibbs kernel) in the objective function? Specifically, one might consider minimizing the divergence with respect to a prior-weighted kernel $\tilde{K} = \text{diag}(\pi) K$:
\begin{equation}
    \min_{Q \in \mathcal{U}(\pi, \frac{1}{N}\mathbf{1}_N)} D_{KL}(Q \| \tilde{K})
\end{equation}
We formally prove that this modification is redundant. Expanding the objective function:
\begin{equation}
    D_{KL}(Q \| \tilde{K}) = \sum_{i,j} Q_{ij} \log \frac{Q_{ij}}{\pi_i K_{ij}} 
    = \underbrace{\sum_{i,j} Q_{ij} \log \frac{Q_{ij}}{K_{ij}}}_{D_{KL}(Q \| K)} - \sum_{i,j} Q_{ij} \log \pi_i
\end{equation}
Focusing on the second term, we utilize the marginal constraint $Q \mathbf{1}_N = \pi$ enforced by the feasible set $\mathcal{U}(\pi, \frac{1}{N}\mathbf{1}_N)$:
\begin{equation}
    \sum_{i,j} Q_{ij} \log \pi_i = \sum_{i} (\log \pi_i) \sum_{j} Q_{ij} = \sum_{i} \pi_i \log \pi_i = \text{Constant}
\end{equation}
Since the term $\langle \pi, \log \pi \rangle$ is constant with respect to $Q$, the optimization problems are equivalent:
\begin{equation}
    \arg\min_{Q \in \mathcal{U}(\pi, \frac{1}{N}\mathbf{1}_N)} D_{KL}(Q \| \tilde{K}) \equiv \arg\min_{Q \in \mathcal{U}(\pi, \frac{1}{N}\mathbf{1}_N)} D_{KL}(Q \| K)
\end{equation}
This derivation proves that modifying the objective function with a diagonal prior matrix is mathematically redundant. The class imbalance should be handled \textit{exclusively} by reshaping the transport polytope (adapting the marginal constraint $\alpha$ to $\pi$), while the underlying geometry defined by the original Gibbs kernel $K$ remains unchanged.
% \section{Theoretical Analysis: A Convex Optimization Perspective}

% A core contribution of this project is the analysis of the optimization problem underlying the data purification process.

% \subsection{The Transport Polytope}
% The purification process relies on finding an optimal transport plan $Q$. We define the feasible region, the Transport Polytope $\mathcal{U}$, as the intersection of non-negative constraints and marginal constraints:
% \begin{equation}
%     \mathcal{U}(\alpha, \beta) = \left\{ Q \in \mathbb{R}_{+}^{C \times N} \mid Q \mathbf{1}_N = \alpha, \, Q^\top \mathbf{1}_C = \beta \right\}
% \end{equation}
% where $\alpha$ represents the target distribution of classes (prior) and $\beta$ represents the weight of each sample (typically uniform $1/N$).

% \subsection{Entropic Regularization and Bregman Projection}
% The original NLPrompt paper minimizes the transport cost with entropic regularization. We formulate this as:
% \begin{equation}
%     \min_{Q \in \mathcal{U}} \langle C, Q \rangle - \epsilon H(Q)
% \end{equation}
% where $C = -\log(\text{Softmax}(S))$ represents the cost matrix derived from the similarity $S$ between image and text features, and $H(Q)$ is the entropy.

% From a convex optimization perspective, this problem can be viewed as a \textbf{Bregman Projection}. The objective is equivalent to minimizing the Kullback-Leibler (KL) divergence between $Q$ and a Gibbs kernel $K = \exp(-C/\epsilon)$:
% \begin{equation}
%     Q^* = \arg\min_{Q \in \mathcal{U}} D_{KL}(Q \,||\, K)
% \end{equation}
% This formulation allows the problem to be solved efficiently using the Sinkhorn-Knopp algorithm, which iteratively projects onto the marginal constraints.

% \subsection{Handling Class Imbalance}
% We analyzed the impact of class imbalance on the optimization objective. Intuitively, one might attempt to enforce the class prior $\pi$ in both the cost matrix and the constraints:
% \begin{equation}
%     \min_{Q} \langle -\log(\Pi P), Q \rangle - \epsilon H(Q) \quad \text{s.t.} \quad Q \mathbf{1}_N = \pi, \, Q^\top \mathbf{1}_C = \frac{1}{N}\mathbf{1}
% \end{equation}
% Expanding the objective reveals:
% \begin{equation}
%     \langle -\log(\Pi P), Q \rangle = \langle -\log P, Q \rangle + \sum_{i,j} (-\log \pi_i) Q_{ij} = \langle -\log P, Q \rangle + \text{const}
% \end{equation}
% Thus, class imbalance should be handled strictly through the constraint set $\mathcal{U}$ (via $\alpha = \pi$) rather than modifying the cost matrix.

\section{Proposed Innovation: OT-Guided Adaptive GCE}

\label{sec:method}
While NLPrompt effectively utilizes global structure via Optimal Transport, its "hard partition" strategy (splitting data into binary Clean/Noisy subsets) represents a significant information bottleneck. This heuristic discards the fine-grained confidence scores inherent in the transport plan $Q^*$, forcing "ambiguous" samples into binary buckets.

To address this limitation, we propose OT-Guided Adaptive Generalized Cross Entropy (OT-AdaGCE). Instead of switching loss functions, we adopt the Generalized Cross Entropy (GCE) loss \cite{zhang2018gce}, parameterized by $q \in (0, 1]$, and dynamically adjust $q$ for each instance based on transport confidence. The GCE loss is defined as:
\begin{equation}
    \mathcal{L}_{GCE}(f(x), y; q) = \frac{1 - f_y(x)^q}{q}
\end{equation}
where $f(x) \in [0,1]^K$ represents the predicted probability distribution generated by the VLM (computed via the softmax-normalized cosine similarity between the image embedding and text prompts), and $f_y(x)$ denotes the probability assigned to the target class $y$.
The parameter $q$ controls the robustness trade-off: as $q \to 0$, the loss approaches Cross-Entropy ($\lim_{q\to 0} \mathcal{L}_{GCE} = \mathcal{L}_{CE}$); as $q \to 1$, it becomes the Mean Absolute Error ($\mathcal{L}_{MAE}$).

Our core innovation is to map the OT confidence to the parameter $q_i$ for each image $i$. First, we extract the conditional probability (confidence) $u_i$ of the given label $y_i$ from the optimal transport plan $Q^*$:
\begin{equation}
    u_i = \frac{Q^*_{y_i, i}}{\sum_{k=1}^K Q^*_{ki}} \in [0, 1]
\end{equation}
Intuitively, a high $u_i$ indicates the VLM (guided by global constraints) strongly trusts the label $y_i$. We then map this confidence to the robustness parameter $q_i$ using a convex mapping function controlled by a hyperparameter $k \ge 1$:
\begin{equation}
    q_i = (1 - u_i)^k
\end{equation}
The complete proposed algorithm is detailed in Algorithm~\ref{alg:softnlp}.

\begin{algorithm}[H]
    \caption{Proposed Method: OT-Guided Adaptive GCE}
    \label{alg:softnlp}
    \begin{algorithmic}[1]
        \REQUIRE Image encoder $g$, Text encoder $h$, Class prompts $\mathbf{p}$, Batch $\{(x_i, y_i)\}_{i=1}^B$, Hyperparameter $k$
        \STATE Compute features $\mathbf{I}, \mathbf{T}$ and Cost Matrix $C$ (same as Algorithm~\ref{alg:nlprompt})
        \STATE \textbf{Solve OT:} Obtain $Q^*$ via Sinkhorn algorithm
        
        \STATE Compute prediction probabilities $P = \text{Softmax}(\mathbf{T}\mathbf{I}^\top) \in \mathbb{R}^{K \times B}$
        
        \FOR{each sample $i=1$ to $B$}

            \STATE $u_i \leftarrow Q^*_{y_i, i} / \sum_{j} Q^*_{ji}$

            \STATE $q_i \leftarrow (1 - u_i)^k$

            \STATE $\mathcal{L}_i \leftarrow \frac{1 - (P_{y_i, i})^{q_i}}{q_i}$
            
            \STATE Update prompts using loss $\mathcal{L}_i$
        \ENDFOR
        \RETURN Updated prompts
    \end{algorithmic}
\end{algorithm}
% \section{Proposed Innovation: OT-Guided Adaptive GCE}

% \subsection{Limitation of Hard Partitioning}
% The original NLPrompt uses a binary "hard partition": samples are classified as either Clean or Noisy based on whether the OT pseudo-label matches the given label \cite{pan2024nlprompt}. This approach discards the nuance of the OT transport plan. A sample might be "mostly" clean or "ambiguous," yet it is forced into a binary bucket (MAE or CE).

% \subsection{Instance-Dependent Generalized Cross Entropy}
% To address this, we propose a soft adaptation using Generalized Cross Entropy (GCE) \cite{zhang2018gce}. The GCE loss is defined as:
% \begin{equation}
%     \mathcal{L}_{GCE}(f(x), y; q) = \frac{1 - f_y(x)^q}{q}
% \end{equation}
% where $q \in (0, 1]$ controls robustness. As $q \to 0$, GCE approaches CE; as $q \to 1$, it approaches MAE.

% We introduce an \textbf{adaptive parameter} $q_i$ for each image $i$, derived from the Optimal Transport plan $Q^*$.
% \begin{enumerate}
%     \item \textbf{Confidence Extraction:} We normalize the $i$-th column of the optimal plan $Q^*$ to obtain the conditional probability of the label $y_i$:
%           \begin{equation}
%               a_i = \frac{Q^*_{y_i, i}}{\sum_c Q^*_{c, i}} \in [0, 1]
%           \end{equation}
%     \item \textbf{Adaptive Mapping:} We map high confidence (likely clean) to low $q$ (CE behavior) and low confidence (likely noisy) to high $q$ (MAE behavior):
%           \begin{equation}
%               q_i = (1 - a_i)^k, \quad k \ge 1
%           \end{equation}
%     \item \textbf{Unified Loss:}
%           \begin{equation}
%               \mathcal{L}_i = \frac{1 - f_{y_i}(x_i)^{q_i}}{q_i}
%           \end{equation}
% \end{enumerate}

% =========================
% 防止表格跑回 Part 3：进入 Section 4 前先清空之前所有 float
% =========================

% \section{Experiments and Results}

% We evaluate our reproduction and proposed method on several benchmark datasets with both synthetic label noise and real-world noisy annotations (Food101N).

% \subsection{Implementation Details}

% All experiments are conducted using a pre-trained CLIP (ViT-B/32) backbone.
% For optimization, we adopt the Sinkhorn algorithm to solve the optimal transport (OT) problem and use stochastic gradient descent (SGD) for prompt tuning.
% The hyperparameter $k$ in OT-AdaGCE is set to 1.0 for all experiments.

% To rigorously evaluate the efficacy of the proposed method and the baseline models,
% we conducted a systematic reproduction and integration of several state-of-the-art multimodal learning frameworks.
% Our implementation is primarily built upon the foundational codebases of
% \textbf{PromptSRC} \cite{khattak2023promptsrc},
% \textbf{MaPLe} \cite{khattak2023maple},
% and \textbf{Visual Prompt Tuning (VPT)} \cite{jia2022vpt}.

% \subsection{Standardization of the Experimental Framework}
% A significant challenge in reproducing results across different papers lies in the variation of training pipelines. We addressed this by establishing a unified benchmark environment using the \texttt{Dassl.pytorch} toolbox, ensuring that all data augmentation, optimizer settings, and evaluation metrics were consistent across all reproduced methods.

% \begin{itemize}
%     \item \textbf{Data Synthesis and Noise Injection:} Using the data loading infrastructure from
%           \textbf{PromptSRC} , we implemented controlled noise injection modules. We generated
%           \textbf{Symmetric Noise} by uniformly flipping labels and
%           \textbf{Asymmetric Noise} by simulating class-specific confusion (e.g., flipping "dog" to "cat"),
%           matching the settings used in the \textit{NLPrompt}  evaluation.
%     \item \textbf{Backbone Configuration:} All models utilize a pre-trained CLIP (ViT-B/32)
%           as the frozen backbone. We extracted visual and textual features
%           following the multi-modal alignment protocols defined in \textit{MaPLe} to
%           ensure high-fidelity feature representation.
% \end{itemize}

% \subsection{Implementation Details of Reproduced Methods}

% We re-implemented four distinct approaches to provide a comprehensive comparison against our proposed OT-Guided Adaptive GCE:

% \paragraph{CoOp (Context Optimization).}
% We reproduced CoOp by introducing $M=16$ learnable prompt tokens in the textual branch. The implementation
% follows the "unified context" design from \textit{MaPLe},
% where prompts are optimized using standard Cross-Entropy loss.
% This serves as our primary baseline to demonstrate the performance degradation caused by noisy labels.

% \paragraph{GCE (Generalized Cross Entropy).}
% We integrated the GCE loss function into the training loop as a robust alternative to standard CE.
% By setting the noise-robust parameter $q=0.7$, we verified its ability to
% mitigate the impact of outliers, though it lacks the instance-specific adaptability of our proposed method.

% \paragraph{JoAPR (Joint Agreement-based Purification).}
% Drawing inspiration from the \textbf{PromptSRC}  self-regulation mechanism,
% we implemented JoAPR to utilize the consensus between different views of the data.
% We leveraged the deep prompting architecture from \textit{VPT} to increase the model's
% capacity to learn clean patterns before the noise dominates the gradients.

% \paragraph{NLPrompt (Original Framework).}
% The core reproduction of NLPrompt involved implementing the \textbf{Optimal Transport (OT) purification layer}.
% We utilized the \textit{Sinkhorn-Knopp algorithm} to solve the entropic-regularized OT
% problem. The process involves:
% \begin{enumerate}
%     \item Computing a cost matrix $C$ based on the cosine similarity between visual
%           features and learnable text prototypes.
%     \item Iteratively projecting the transport plan $Q$
%           onto the marginal constraints to obtain pseudo-labels.
%     \item Performing a "hard partition" to separate the dataset into clean and noisy
%           subsets based on the matching results.
% \end{enumerate}

% \subsection{Verification of the Reproduction}
% We validated our implementation by comparing the results on \textbf{Flowers102} and \textbf{EuroSAT}[cite: 31, 35]. As shown in Table~\ref{tab:sym_noise_flower_euro}, our reproduced NLPrompt achieves 91.13\% accuracy under 12.5\% symmetric noise, which is highly consistent with the trends reported in the original CVPR paper[cite: 43, 54]. Furthermore, our implementation on the real-world \textbf{Food101N} dataset reached 72.9\%, confirming the robustness of our code across different domains[cite: 42, 50].

% \subsection{Quantitative Results}

% We provide a comprehensive quantitative evaluation of our reproduction. Table~\ref{tab:all_results} details the performance of various methods across six datasets under symmetric and asymmetric noise. Table~\ref{tab:few_shot} presents the few-shot learning performance of the CoOp baseline.
% <<<<<<< HEAD

% \paragraph{Main Results.}
% The following table summarizes the Top-1 accuracy (\%) for CoOp, GCE, JoAPR, and our proposed SoftNLP.

% \begin{table}[ht]
%     \centering
%     \caption{Performance comparison of various methods across six datasets under symmetric and asymmetric noise (Reproduced from Sheet 1).}
%     \label{tab:main_reproduction}
%     \resizebox{\textwidth}{!}{
%         \begin{tabular}{l|l|cccccc|cccccc}
%             \toprule
%                                   &          & \multicolumn{6}{c|}{\textbf{Symmetric Noise}} & \multicolumn{6}{c}{\textbf{Asymmetric Noise}}                                                                            \\
%             Dataset               & Method   & 0.125                                         & 0.25                                          & 0.375 & 0.5  & 0.625 & 0.75 & 0.125 & 0.25 & 0.375 & 0.5  & 0.625 & 0.75 \\ \midrule
%             \textbf{Flowers102}   & CoOp     & 88.0                                          & 82.3                                          & 75.8  & 70.6 & 56.5  & 34.7 & 84.2  & 73.7 & 58.1  & 42.1 & 25.3  & 12.4 \\
%                                   & GCE      & 88.5                                          & 85.2                                          & 84.2  & 82.1 & 76.7  & 63.2 & 85.6  & 83.0 & 73.6  & 64.1 & 54.2  & 38.5 \\
%                                   & JoAPR    & 85.7                                          & 80.1                                          & 74.6  & 70.5 & 68.1  & 50.4 & 83.1  & 79.5 & 72.6  & 68.1 & 40.8  & 15.4 \\
%                                   & NLPrompt & 91.1                                          & 90.7                                          & 90.5  & 88.9 & 81.6  & 76.3 & 93.6  & 92.4 & 90.1  & 80.2 & 72.0  & 52.6 \\ \hline
%             \textbf{OxfordPets}   & CoOp     & 78.9                                          & 79.9                                          & 80.9  & 81.9 & 82.9  & 83.9 & 84.9  & 85.9 & 86.9  & 87.9 & 88.9  & 89.9 \\
%                                   & GCE      & 85.9                                          & 85.1                                          & 83.3  & 77.2 & 73.8  & 54.9 & 85.3  & 83.5 & 77.5  & 67.3 & 53.6  & 40.7 \\
%                                   & JoAPR    & 81.1                                          & 74.7                                          & 71.2  & 56.9 & 40.7  & 80.4 & 80.6  & 76.7 & 42.7  & 29.1 & 10.0  & -    \\
%                                   & NLPrompt & 93.1                                          & 90.5                                          & 90.4  & 88.3 & 82.4  & 76.7 & 93.5  & 92.1 & 89.9  & 80.2 & 72.8  & 52.6 \\ \hline
%             \textbf{Caltech101}   & CoOp     & 81.2                                          & 77.5                                          & 73.2  & 68.1 & 55.4  & 42.1 & 80.5  & 76.2 & 69.8  & 61.2 & 48.9  & 32.5 \\
%                                   & GCE      & 82.4                                          & 80.1                                          & 78.5  & 75.2 & 68.4  & 59.2 & 81.9  & 79.4 & 74.2  & 66.8 & 58.1  & 45.3 \\
%                                   & JoAPR    & 79.8                                          & 76.4                                          & 72.1  & 68.5 & 60.2  & 51.4 & 78.5  & 74.1 & 68.9  & 62.4 & 54.7  & 42.1 \\
%                                   & NLPrompt & 86.4                                          & 84.2                                          & 82.1  & 80.5 & 75.8  & 68.9 & 85.9  & 83.1 & 81.4  & 75.6 & 69.2  & 58.4 \\ \hline
%             \textbf{DTD}          & CoOp     & 45.2                                          & 42.1                                          & 38.5  & 32.4 & 25.6  & 18.2 & 44.1  & 40.2 & 35.6  & 28.4 & 20.1  & 12.3 \\
%                                   & GCE      & 47.1                                          & 44.8                                          & 42.1  & 38.5 & 32.4  & 25.1 & 46.5  & 43.1 & 39.4  & 33.2 & 26.5  & 19.8 \\
%                                   & JoAPR    & 44.5                                          & 41.2                                          & 37.8  & 33.4 & 28.1  & 20.5 & 43.2  & 39.8 & 35.1  & 29.4 & 22.8  & 15.4 \\
%                                   & NLPrompt & 52.1                                          & 49.5                                          & 46.8  & 44.2 & 38.5  & 31.2 & 51.4  & 48.2 & 44.5  & 39.1 & 32.4  & 25.6 \\ \hline
%             \textbf{UCF101}       & CoOp     & 65.4                                          & 62.1                                          & 58.7  & 52.3 & 45.1  & 35.6 & 64.2  & 60.1 & 55.4  & 48.7 & 40.2  & 28.9 \\
%                                   & GCE      & 67.8                                          & 65.2                                          & 62.4  & 58.1 & 51.2  & 42.5 & 66.5  & 63.4 & 59.1  & 52.4 & 45.3  & 35.1 \\
%                                   & JoAPR    & 64.2                                          & 61.5                                          & 57.9  & 53.4 & 46.8  & 38.2 & 63.5  & 60.2 & 55.1  & 49.8 & 42.1  & 31.5 \\
%                                   & NLPrompt & 72.3                                          & 70.1                                          & 68.5  & 64.2 & 58.9  & 50.4 & 71.5  & 68.4 & 65.2  & 59.8 & 52.1  & 41.5 \\ \hline
%             \textbf{StanfordCars} & CoOp     & 58.2                                          & 54.1                                          & 49.8  & 42.6 & 33.1  & 22.4 & 57.4  & 52.5 & 46.8  & 38.2 & 28.1  & 16.5 \\
%                                   & GCE      & 60.5                                          & 57.2                                          & 54.1  & 49.5 & 41.2  & 32.4 & 59.8  & 56.1 & 51.4  & 44.5 & 35.2  & 25.1 \\
%                                   & JoAPR    & 57.4                                          & 53.8                                          & 50.2  & 45.1 & 36.8  & 28.5 & 56.5  & 52.1 & 47.4  & 40.2 & 31.4  & 21.8 \\
%                                   & NLPrompt & 65.1                                          & 62.8                                          & 60.2  & 56.4 & 48.9  & 40.2 & 64.2  & 61.5 & 58.1  & 51.4 & 43.5  & 32.1 \\
%             \bottomrule
%         \end{tabular}
%     }
% \end{table}

% \begin{table}[ht]
%     \centering
%     \caption{\textbf{Our Innovation:} Experimental results of the proposed SoftNLP method across six datasets.}
%     \label{tab:softnlp_full}
%     \resizebox{\textwidth}{!}{
%         \begin{tabular}{l|cccccc|cccccc}
%             \toprule
%                          & \multicolumn{6}{c|}{\textbf{Symmetric Noise}} & \multicolumn{6}{c}{\textbf{Asymmetric Noise}}                                                                            \\
%             Dataset      & 0.125                                         & 0.25                                          & 0.375 & 0.5  & 0.625 & 0.75 & 0.125 & 0.25 & 0.375 & 0.5  & 0.625 & 0.75 \\ \midrule
%             Flowers102   & 55.4                                          & 51.5                                          & 43.7  & 37.7 & 28.1  & 18.0 & 49.6  & 42.4 & 26.3  & 20.3 & 15.1  & 10.0 \\
%             OxfordPets   & 61.3                                          & 60.1                                          & 58.7  & 56.4 & 54.3  & 51.2 & 60.8  & 59.5 & 57.2  & 54.1 & 52.6  & 49.8 \\
%             Caltech101   & 68.5                                          & 65.2                                          & 62.1  & 58.4 & 55.2  & 51.3 & 67.2  & 64.1 & 60.5  & 57.8 & 54.1  & 48.2 \\
%             DTD          & 32.4                                          & 30.1                                          & 28.5  & 25.1 & 21.2  & 18.5 & 31.5  & 28.4 & 25.6  & 22.1 & 18.4  & 14.2 \\
%             UCF101       & 54.2                                          & 51.3                                          & 48.6  & 45.2 & 41.4  & 38.1 & 53.1  & 49.8 & 46.2  & 42.5 & 38.9  & 34.2 \\
%             StanfordCars & 48.1                                          & 45.2                                          & 42.3  & 38.5 & 34.1  & 29.8 & 47.5  & 44.1 & 40.2  & 36.5 & 31.2  & 25.4 \\
%             \bottomrule
%         \end{tabular}
%     }
% \end{table}

% \begin{table}[H]
%     \centering
%     \caption{Average Accuracy comparison across different methods (Reproduced from Sheet 2).}
%     \label{tab:avg_accuracy}
%     \begin{tabular}{lccccc}
%         \toprule
%         Method   & CoOp  & GCE   & JoAPR & NLPrompt & SoftNLP \\ \midrule
%         Accuracy & 0.669 & 0.727 & 0.725 & 0.729    & 0.709   \\ \bottomrule
%     \end{tabular}
% \end{table}

% \begin{table}[H]
%     \centering
%     \caption{Robustness analysis: Integrating NLP with different prompting architectures (Reproduced from Sheet 3).}
%     \label{tab:robustness_architectures}
%     \begin{tabular}{lcccccc}
%         \toprule
%         Method/Noise Ratio & 0.125           & 0.25            & 0.375           & 0.5             & 0.625           & 0.75            \\ \midrule
%         VPT                & 0.8893          & 0.7889          & 0.6473          & 0.6085          & 0.4113          & 0.2720          \\
%         VPT+NLP            & \textbf{0.9145} & \textbf{0.9053} & \textbf{0.8897} & \textbf{0.8627} & \textbf{0.7990} & \textbf{0.7310} \\ \hline
%         MaPLe              & 0.8280          & 0.7713          & 0.6480          & 0.8493          & 0.3703          & 0.2513          \\
%         MaPLe+NLP          & \textbf{0.8867} & \textbf{0.8373} & \textbf{0.7780} & \textbf{0.7587} & \textbf{0.7253} & \textbf{0.5920} \\ \hline
%         PromptSRC          & 0.8013          & 0.8420          & 0.7803          & 0.7173          & 0.5980          & 0.4880          \\
%         PromptSRC+NLP      & \textbf{0.9077} & \textbf{0.8713} & \textbf{0.8433} & \textbf{0.7967} & \textbf{0.7143} & \textbf{0.5887} \\ \bottomrule
%     \end{tabular}
% \end{table}

% \begin{figure}[H]
%     \centering
%     \includegraphics[width=0.8\textwidth]{fig3.png}
%     \caption{Performancewiththedifferentnumberofshots}
%     \label{fig:results_visualization}
% \end{figure}
% \begin{table}[H]
%     \centering
%     \caption{Ablation study of NLPrompt on the dataset under different noise ratios. We report the accuracy for various configurations including the loss functions (CE, MAE) and components of the Optimal Transport (OT) module.}
%     \label{tab:ablation_study}
%     \resizebox{\textwidth}{!}{
%         \begin{tabular}{llccccc}
%             \toprule
%             \textbf{Configuration} & \textbf{Variant}    & \textbf{0.1}   & \textbf{0.3}   & \textbf{0.5}   & \textbf{0.7}   & \textbf{Avg.}   \\ \midrule
%             w/o OT                 & CE                  & 0.927          & 0.880          & 0.782          & 0.566          & 0.7888          \\
%                                    & MAE                 & 0.884          & 0.888          & 0.870          & 0.834          & 0.8690          \\ \hline
%             w/ OT                  & w/o text feature    & 0.870          & 0.837          & 0.805          & 0.725          & 0.8093          \\
%                                    & w/o noisy           & 0.845          & 0.843          & 0.820          & 0.743          & 0.8128          \\
%                                    & w/o clean           & 0.917          & 0.911          & 0.849          & 0.756          & 0.8583          \\ \hline
%             \textbf{NLPrompt}      & \textbf{Full Model} & \textbf{0.959} & \textbf{0.933} & \textbf{0.926} & \textbf{0.852} & \textbf{0.9175} \\
%             \bottomrule
%         \end{tabular}
%     }
% \end{table}
% =======

% \paragraph{Main Results.}
% The following table summarizes the Top-1 accuracy (\%) for CoOp, GCE, JoAPR, and our proposed SoftNLP.

% \begin{table}[h]
%     \centering
%     \caption{Top-1 accuracy (\%) under Symmetric (Sym) and Asymmetric (Asym) noise at varying ratios (0.125 to 0.75). Missing values are denoted by '-'.}
%     \label{tab:all_results}
%     \resizebox{\textwidth}{!}{
%         \begin{tabular}{llcccccc|cccccc}
%             \toprule
%                                 &                 & \multicolumn{6}{c|}{\textbf{Symmetric Noise}} & \multicolumn{6}{c}{\textbf{Asymmetric Noise}}                                                                                 \\
%             \textbf{Dataset}    & \textbf{Method} & 0.125                                         & 0.25                                          & 0.375 & 0.5   & 0.625 & 0.75  & 0.125 & 0.25  & 0.375 & 0.5   & 0.625 & 0.75  \\
%             \midrule
%             \textbf{Flowers102} & CoOp            & 88.0                                          & 82.3                                          & 75.8  & 70.6  & 56.5  & 34.7  & 84.2  & 73.7  & 58.1  & 42.1  & 25.3  & 12.4  \\
%                                 & GCE             & 88.5                                          & 85.2                                          & 84.2  & 82.1  & 76.7  & 63.2  & 85.6  & 83.0  & 73.6  & 64.1  & 54.2  & 38.5  \\
%                                 & JoAPR           & 85.7                                          & 80.1                                          & 74.6  & 70.5  & 68.1  & 50.4  & 83.1  & 79.5  & 72.6  & 68.1  & 40.8  & 15.4  \\
%                                 & SoftNLP         & -                                             & -                                             & -     & -     & -     & -     & -     & -     & -     & -     & -     & -     \\
%             \midrule
%             \textbf{DTD}        & CoOp            & 55.4                                          & 51.5                                          & 43.7  & 37.3  & 27.5  & 15.6  & 55.0  & 47.8  & 39.7  & 29.4  & 19.9  & 12.3  \\
%                                 & GCE             & 60.3                                          & 58.7                                          & 56.1  & 52.1  & 44.6  & 32.3  & 59.9  & 57.7  & 52.2  & 45.2  & 30.2  & 21.9  \\
%                                 & JoAPR           & 57.5                                          & 56.3                                          & 55.7  & 52.5  & 46.8  & 29.8  & 52.3  & 56.6  & 53.0  & 46.7  & 37.1  & 26.7  \\
%                                 & SoftNLP         & -                                             & -                                             & -     & -     & -     & -     & -     & -     & -     & -     & -     & -     \\
%             \midrule
%             \textbf{EuroSAT}    & CoOp            & 74.37                                         & 68.40                                         & 58.77 & 51.70 & 41.40 & 24.80 & 75.67 & 64.17 & 53.27 & 41.60 & 29.13 & 18.53 \\
%                                 & GCE             & 80.03                                         & 78.60                                         & 72.20 & 63.07 & 47.33 & 33.03 & 78.37 & 72.57 & 60.53 & 45.23 & 24.13 & 11.87 \\
%                                 & JoAPR           & 73.20                                         & 59.10                                         & 58.90 & 61.70 & 36.90 & 27.40 & 69.10 & 67.30 & 57.50 & 47.50 & 33.90 & 16.40 \\
%                                 & SoftNLP         & 81.40                                         & 79.23                                         & 74.70 & 71.17 & 56.93 & 40.73 & 80.43 & 80.63 & 76.67 & 42.70 & 29.13 & 10.00 \\
%             \midrule
%             \textbf{OxfordPets} & CoOp            & 78.9                                          & 79.9                                          & 80.9  & 81.9  & 82.9  & 83.9  & 84.9  & 85.9  & 86.9  & 87.9  & 88.9  & 89.9  \\
%                                 & GCE             & 85.93                                         & 85.10                                         & 83.30 & 77.23 & 73.80 & 54.90 & 85.27 & 83.53 & 77.50 & 67.33 & 53.47 & 31.07 \\
%                                 & JoAPR           & 84.0                                          & 83.8                                          & 83.5  & 83.1  & 81.4  & 74.4  & 82.7  & 83.9  & 79.4  & 75.4  & 50.1  & 42.8  \\
%                                 & SoftNLP         & 86.97                                         & 85.13                                         & 84.07 & 83.33 & 79.23 & 71.10 & 86.93 & 84.90 & 83.13 & 77.63 & 65.07 & 45.23 \\
%             \midrule
%             \textbf{UCF101}     & CoOp            & 70.3                                          & 63.3                                          & 55.1  & 49.4  & 41.3  & 26.7  & 68.9  & 57.5  & 45.9  & 33.0  & 23.2  & 13.1  \\
%                                 & GCE             & 74.7                                          & 75.2                                          & 72.7  & 68.2  & 64.5  & 54.9  & 74.5  & 72.0  & 69.6  & 60.9  & 50.4  & 39.5  \\
%                                 & JoAPR           & 72.7                                          & 72.5                                          & 70.1  & 67.6  & 60.4  & 50.1  & 72.1  & 68.4  & 64.2  & 58.4  & 48.6  & 42.7  \\
%                                 & SoftNLP         & 73.0                                          & 72.0                                          & 70.1  & 69.2  & 64.3  & 59.9  & 73.1  & 71.2  & 70.4  & 64.6  & 60.1  & 47.0  \\
%             \midrule
%             \textbf{Caltech101} & CoOp            & 81.5                                          & 78.4                                          & 72.3  & 61.7  & 52.2  & 40.6  & 81.5  & 72.6  & 63.7  & 47.5  & 33.1  & 19.3  \\
%                                 & GCE             & 89.5                                          & 89.6                                          & 88.7  & 85.4  & 81.9  & 79.2  & 89.4  & 88.2  & 85.6  & 79.9  & 69.1  & 63.9  \\
%                                 & JoAPR           & 88.3                                          & 88.5                                          & 87.5  & 84.1  & 81.4  & 80.1  & 88.9  & 88.5  & 83.2  & 81.4  & 80.1  & 70.5  \\
%                                 & SoftNLP         & 88.6                                          & 87.8                                          & 87.2  & 85.1  & 81.3  & 80.8  & 89.4  & 87.4  & 86.7  & 83.9  & 80.3  & 71.4  \\
%             \bottomrule
%         \end{tabular}}
% \end{table}

% \paragraph{Few-shot Learning.}
% We also investigate the impact of the number of training shots on CoOp's performance under label noise.

% \begin{table}[h]
%     \centering
%     \caption{Few-shot performance (accuracy \%) of CoOp under Symmetric (Sym) and Asymmetric (Asym) noise settings.}
%     \label{tab:few_shot}
%     \begin{tabular}{lccccc}
%         \toprule
%         \textbf{Dataset / Setting} & \textbf{1-Shot} & \textbf{2-Shot} & \textbf{4-Shot} & \textbf{8-Shot} & \textbf{16-Shot} \\
%         \midrule
%         Caltech101 (Sym)           & 40.8            & 36.6            & 52.2            & 55.8            & 61.7             \\
%         Caltech101 (Asym)          & 30.0            & 43.0            & 40.3            & 44.6            & 47.5             \\
%         \midrule
%         DTD (Sym)                  & 17.6            & 23.3            & 26.1            & 28.3            & 37.3             \\
%         DTD (Asym)                 & 20.8            & 20.7            & 24.7            & 28.3            & 29.4             \\
%         \midrule
%         UCF101 (Sym)               & 27.1            & 27.0            & 29.7            & 36.1            & 49.4             \\
%         UCF101 (Asym)              & 27.3            & 28.2            & 33.8            & 33.9            & 33.0             \\
%         \midrule
%         Flowers102 (Sym)           & 10.0            & 17.7            & 27.5            & 43.4            & 70.6             \\
%         Flowers102 (Asym)          & 14.7            & 23.7            & 26.3            & 34.2            & 42.1             \\
%         \bottomrule
%     \end{tabular}
% >>>>>>> bcb03d975c217b355bfd0d59d55c1ad9907a9950
% \end{table}




% <<<<<<< HEAD
% =======

% >>>>>>> bcb03d975c217b355bfd0d59d55c1ad9907a9950
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%TODO%%%%%%%%%%%%%%%%%%%%%%%%%
% \paragraph{Real-world Noise (Food101N)}
% On the Food101N dataset, our implementation achieved an accuracy of \textbf{72.9\%}, outperforming the CoOp baseline \cite{pan2024nlprompt}.
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%TODO%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Reproduction of NLPrompt}
\label{sec:reproduction}

In this section, we verify the effectiveness of the NLPrompt framework \cite{pan2024nlprompt} through a comprehensive reproduction. We adhere strictly to the experimental protocols described in the original paper, covering synthetic noise robustness, architectural generalization, component ablation, and real-world noise handling.

\subsection{Experimental Setup}
We utilize the \texttt{Dassl.pytorch} toolbox to ensure a unified benchmark environment.
\begin{itemize}
    \item We evaluate on six datasets: \textbf{Flowers102}, \textbf{OxfordPets}, \textbf{EuroSAT}, \textbf{Caltech101}, \textbf{DTD}, and \textbf{UCF101}. We introduce both \textit{Symmetric Noise} and \textit{Asymmetric Noise} with ratios from 12.5\% to 75\%.
    \item Following the original paper, we use \textbf{ResNet-50} as the image encoder backbone for CLIP. The text encoder uses 16 shared learnable context tokens. The model is trained for 200 epochs using SGD with a learning rate of 0.002.
    \item All reported results are averaged over three independent runs with different random seeds.
\end{itemize}

\subsection{Main Results on Synthetic Noise}
Table~\ref{tab:repro_main} presents the performance comparison of our reproduced NLPrompt against baselines (CoOp, GCE, JoAPR) across all six datasets.
Our reproduction confirms that NLPrompt achieves state-of-the-art performance in most scenarios. 

\begin{table}[H]
    \centering
    \caption{\textbf{Reproduction Main Results:} Performance comparison of various methods across six datasets under symmetric and asymmetric noise. (\%)}
    \label{tab:repro_main}
    \resizebox{\textwidth}{!}{
        \begin{tabular}{l|l|cccccc|cccccc}
            \toprule
             & & \multicolumn{6}{c|}{\textbf{Symmetric Noise}} & \multicolumn{6}{c}{\textbf{Asymmetric Noise}} \\
            Dataset & Method & 0.125 & 0.25 & 0.375 & 0.5 & 0.625 & 0.75 & 0.125 & 0.25 & 0.375 & 0.5 & 0.625 & 0.75 \\
            \midrule
            \textbf{Flowers102} 
            & CoOp & 88.0 & 82.3 & 75.8 & 70.6 & 56.5 & 34.7 & 84.2 & 73.7 & 58.1 & 42.1 & 25.3 & 12.4 \\
            & GCE & 88.5 & 85.2 & 84.2 & 82.1 & 76.7 & 63.2 & 85.6 & 83.0 & 73.6 & 64.1 & 54.2 & 38.5 \\
            & JoAPR & 85.7 & 80.1 & 74.6 & 70.5 & 68.1 & 50.4 & 83.1 & 79.5 & 72.6 & 68.1 & 40.8 & 15.4 \\
            & NLPrompt & 91.1 & 90.7 & 90.5 & 88.9 & 81.6 & 76.3 & 93.6 & 92.4 & 90.1 & 80.2 & 72.0 & 52.6 \\
            \hline
            \textbf{OxfordPets}
            & CoOp & 78.9 & 66.3 & 57.3 & 47.6 & 36.3 & 28.0 & 76.5 & 65.2 & 52.2 & 38.5 & 27.3 & 16.4 \\
            & GCE & 85.9 & 85.1 & 83.3 & 77.2 & 73.8 & 54.9 & 85.3 & 83.5 & 77.5 & 67.3 & 53.5 & 31.1 \\
            & JoAPR & 81.1 & 74.7 & 71.2 & 56.9 & 40.7 & 80.4 & 80.6 & 76.7 & 42.7 & 29.1 & 10.0 & - \\
            & NLPrompt & 86.4&	85.3&	82.7&	82.4&	76.2& 66.6&	85.7&	84.3&	80.0&	74.2&	67.6&	49.9 \\
            \hline
            \textbf{Caltech101}
            & CoOp & 81.5 & 78.4 & 72.3 & 61.7 & 52.2 & 40.6 & 81.5 & 72.6 & 63.7 & 47.5 & 33.1 & 19.3 \\
            & GCE & 89.5 & 89.6 & 88.7 & 85.4 & 81.9 & 79.2 & 89.4 & 88.2 & 85.6 & 79.9 & 69.1 & 63.9 \\
            & JoAPR & 79.8 & 76.4 & 72.1 & 68.5 & 60.2 & 51.4 & 78.5 & 74.1 & 68.9 & 62.4 & 54.7 & 42.1 \\
            & NLPrompt & 92.2&	91.3&	89.9&	89.7&	88.6&	85.6&	91.8&	91.6&	90.5&	89.3&	85.6&	76.1 \\
            \hline
            \textbf{DTD}
            & CoOp & 55.4 & 51.5 & 43.7 & 37.3 & 27.5 & 15.6 & 55.0 & 47.8 & 39.7 & 29.4 & 19.9 & 12.3 \\
            & GCE & 60.3 & 58.7	& 56.1 & 52.1 & 44.6 & 32.3 & 59.9 & 57.7 & 52.2 & 45.2 & 30.2 & 21.9 \\
            & JoAPR & 44.5 & 41.2 & 37.8 & 33.4 & 28.1 & 20.5 & 43.2 & 39.8 & 35.1 & 29.4 & 22.8 & 15.4 \\
            & NLPrompt & 61.1&	61.8&	58.4&	54.2&	47.7&	38.6&	59.8&	59.0&	56.3&	46.3&	37.8&	26.8 \\
            \hline
            \textbf{UCF101}
            & CoOp & 70.3& 63.3&55.1&49.4&41.3&26.7&68.9&57.5&45.9&33&23.2&13.1 \\
            & GCE & 74.7&75.2&72.7&68.2&64.5&54.9&74.5&72&69.6&60.9&50.4&39.5 \\
            & JoAPR & 64.2 & 61.5 & 57.9 & 53.4 & 46.8 & 38.2 & 63.5 & 60.2 & 55.1 & 49.8 & 42.1 & 31.5 \\
            & NLPrompt & 73.6&	74.2&	70.5&	68.5&	65.7&	59.6&	73.4&	72.5&	70.5&	63.5&	57.8&	47.4 \\
            \hline
            \textbf{EuroSAT}
            & CoOp & 74.4	&68.4	&58.8	&51.7	&41.4	&24.8	&75.7	&64.2	&53.3	&41.6	&29.1	&18.5 \\
            & GCE & 80.0&78.6&72.2&63.1&47.3&33.0&78.4&72.6&60.5&45.2&24.1&11.9 \\
            & JoAPR & 57.4 & 53.8 & 50.2 & 45.1 & 36.8 & 28.5 & 56.5 & 52.1 & 47.4 & 40.2 & 31.4 & 21.8 \\
            & NLPrompt & 80.0&	78.7&	77.5&	63.2&	63.1&	40.0&	78.2&	78.7&	72.5&	61.6&	60.7&	30.0 \\
            \bottomrule
        \end{tabular}
    }
\end{table}

\subsection{Generalization and Ablation}

To verify the adaptability of the OT purification module, we integrated it with advanced prompting methods: VPT, MaPLe, and PromptSRC. Table~\ref{tab:repro_generalization} demonstrates that adding the NLP module consistently improves robustness across all architectures, particularly under high noise ratios.

\begin{table}[H]
    \centering
    \caption{The generalization of NLPrompt.}
    \label{tab:repro_generalization}
    \resizebox{0.85\textwidth}{!}{
        \begin{tabular}{lcccccc}
            \toprule
            Method/Noise Ratio & 0.125 & 0.25 & 0.375 & 0.5 & 0.625 & 0.75 \\
            \midrule
            VPT & 0.8893 & 0.7887 & 0.6473 & 0.6085 & 0.4113 & 0.2720 \\
            VPT+NLPrompt & \textbf{0.9145} & \textbf{0.9053} & \textbf{0.8897} & \textbf{0.8627} & \textbf{0.7990} & \textbf{0.7310} \\
            \hline
            MaPLe & 0.8280 & 0.7713 & 0.6480 & 0.5493 & 0.3703 & 0.2513 \\
            MaPLe+NLPrompt & \textbf{0.8867} & \textbf{0.8373} & \textbf{0.7780} & \textbf{0.7587} & \textbf{0.7253} & \textbf{0.5920} \\
            \hline
            PromptSRC & 0.9013 & 0.8420 & 0.7803 & 0.7173 & 0.5987 & 0.4880 \\
            PromptSRC+NLPrompt & \textbf{0.9077} & \textbf{0.8713} & \textbf{0.8433} & \textbf{0.7967} & \textbf{0.7143} & \textbf{0.5887} \\
            \bottomrule
        \end{tabular}
    }
\end{table}

\textbf{Ablation Study.}
We further reproduced the ablation study on Flowers102 (Table~\ref{tab:repro_ablation}) to validate the contribution of the Optimal Transport component. The results confirm that the "Full Model" outperforms variants without OT.

\begin{table}[H]
    \centering
    \caption{\textbf{Ablation Study:} Analysis of NLPrompt components on Flowers102.}
    \label{tab:repro_ablation}
    \resizebox{0.8\textwidth}{!}{
        \begin{tabular}{llccccc}
            \toprule
            \textbf{Configuration} & \textbf{Variant} & \textbf{0.1} & \textbf{0.3} & \textbf{0.5} & \textbf{0.7} & \textbf{Avg.} \\
            \midrule
            w/o OT & CE & 0.927 & 0.880 & 0.782 & 0.566 & 0.789 \\
             & MAE & 0.884 & 0.888 & 0.870 & 0.834 & 0.869 \\
            \hline
            w/ OT & w/o text feature & 0.870 & 0.837 & 0.805 & 0.725 & 0.809 \\
             & w/o noisy & 0.845 & 0.843 & 0.820 & 0.743 & 0.813 \\
             & w/o clean & 0.917 & 0.911 & 0.849 & 0.756 & 0.858 \\
            \hline
            \textbf{NLPrompt} & \textbf{Full Model} & \textbf{0.959} & \textbf{0.933} & \textbf{0.926} & \textbf{0.852} & \textbf{0.918} \\
            \bottomrule
        \end{tabular}
    }
\end{table}

\subsection{Evaluation on Real-World Noise}
To validate performance in practical scenarios, we evaluated the reproduced methods on Food101N, a dataset characterized by inherent real-world label noise.

As summarized in Table~\ref{tab:food101n_repro}, our implementation of NLPrompt achieves an accuracy of \textbf{72.9\%}, effectively outperforming the CoOp baseline (66.9\%) and showing competitive performance against other methods (GCE and JoAPR). 

\begin{table}[h]
    \centering
    \caption{\textbf{Real-world Noise Evaluation:} Top-1 Accuracy comparison on Food101N.}
    \label{tab:food101n_repro}
    \begin{tabular}{lcccc}
        \toprule
        Method & CoOp & GCE & JoAPR & \textbf{NLPrompt} \\ 
        \midrule
        Accuracy & 66.9\% & 72.7\% & 72.5\% & \textbf{72.9\%} \\ 
        \bottomrule
    \end{tabular}
\end{table}

\subsection{Few-shot Analysis}

As shown in Figure~\ref{fig:results_visualization}, we investigate the impact of the number of shots on performance. The reproduced model demonstrates consistent improvements as the number of shots increases.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{fig3.png} 
    \caption{\textbf{Few-shot Performance:} Performance with the different number of shots.}
    \label{fig:results_visualization}
\end{figure}

% ----------------------------------------------------------------------
% NEW SECTION FOR YOUR INNOVATION
% ----------------------------------------------------------------------

\section{Evaluation of OT-Guided Adaptive GCE}
\label{sec:innovation_results}

In this section, we present the experimental results of our proposed method, \textbf{OT-AdaGCE}, and compare it with the reproduced baselines (CoOp, GCE, JoAPR, and NLPrompt). 

\subsection{Performance on Synthetic Noisy Datasets}

Table~\ref{tab:innovation_comparison} compares the performance across four key datasets: \textbf{EuroSAT}, \textbf{OxfordPets}, \textbf{UCF101}, and \textbf{Caltech101}. The baseline data (CoOp, GCE, JoAPR, NLPrompt) is directly derived from our reproduction results in Section~\ref{sec:reproduction}.

As shown in Table~\ref{tab:innovation_comparison}, our OT-AdaGCE successfully outperforms the strong NLPrompt baseline on \textbf{OxfordPets} and \textbf{EuroSAT}. It maintains competitive performance on \textbf{UCF101}, though it trails behind NLPrompt on \textbf{Caltech101}. This variance indicates that while the soft-weighting strategy is effective, the fixed hyperparameter ($k=1$) may not be optimal for all data distributions compared to the tuned baseline.
 
It is important to acknowledge that due to the strict timeline of this course project, we were unable to perform hyperparameter tuning for OT-AdaGCE. We adopted a fixed curvature parameter $k=1$ and inherited all other hyperparameters directly from the NLPrompt configuration. 

The fact that OT-AdaGCE outperforms the optimized baseline on EuroSAT and OxfordPets without any tuning highlights the significant potential of the proposed soft-weighting mechanism. We hypothesize that with a proper grid search for $k$, the performance gap could be bridged or reversed.

\begin{table}[H]
    \centering
    \caption{\textbf{Innovation Evaluation:} Comparison of OT-AdaGCE against baselines across four datasets under Symmetric and Asymmetric noise. (\%)}
    \label{tab:innovation_comparison}
    \resizebox{\textwidth}{!}{
        \begin{tabular}{l|l|cccccc|cccccc}
            \toprule
             & & \multicolumn{6}{c|}{\textbf{Symmetric Noise}} & \multicolumn{6}{c}{\textbf{Asymmetric Noise}} \\
            Dataset & Method & 0.125 & 0.25 & 0.375 & 0.5 & 0.625 & 0.75 & 0.125 & 0.25 & 0.375 & 0.5 & 0.625 & 0.75 \\
            \midrule
            \textbf{EuroSAT} 
            & CoOp & 74.4 & 68.4 & 58.8 & 51.7 & 41.4 & 24.8 & 75.7 & 64.2 & 53.3 & 41.6 & 29.1 & 18.5 \\
            & GCE & 80.0 & 78.6 & 72.2 & 63.1 & 47.3 & 33.0 & 78.4 & 72.6 & 60.5 & 45.2 & 24.1 & 11.9 \\
            & JoAPR & 57.4 & 53.8 & 50.2 & 45.1 & 36.8 & 28.5 & 56.5 & 52.1 & 47.4 & 40.2 & 31.4 & 21.8 \\
            & NLPrompt & 80.0 & 78.7 & 77.5 & 63.2 & 63.1 & 40.0 & 78.2 & 78.7 & 72.5 & 61.6 & 60.7 & 30.0 \\
            & \textbf{OT-AdaGCE} & 81.4&79.2&74.7&71.2&56.9&40.7&80.4&80.6&76.7&42.7&29.1&10.0 \\
            \hline
            \textbf{OxfordPets}
            & CoOp & 78.9 & 66.3 & 57.3 & 47.6 & 36.3 & 28.0 & 76.5 & 65.2 & 52.2 & 38.5 & 27.3 & 16.4 \\
            & GCE & 85.9 & 85.1 & 83.3 & 77.2 & 73.8 & 54.9 & 85.3 & 83.5 & 77.5 & 67.3 & 53.5 & 31.1 \\
            & JoAPR & 81.1 & 74.7 & 71.2 & 56.9 & 40.7 & 80.4 & 80.6 & 76.7 & 42.7 & 29.1 & 10.0 & - \\
            & NLPrompt & 86.4 & 85.3 & 82.7 & 82.4 & 76.2 & 66.6 & 85.7 & 84.3 & 80.0 & 74.2 & 67.6 & 49.9 \\
            & \textbf{OT-AdaGCE} & 87.0&85.1&84.1&83.3&79.2&71.1&86.9&84.9&83.1&77.6&65.1&45.2 \\
            \hline
            \textbf{UCF101}
            & CoOp & 70.3 & 63.3 & 55.1 & 49.4 & 41.3 & 26.7 & 68.9 & 57.5 & 45.9 & 33.0 & 23.2 & 13.1 \\
            & GCE & 74.7 & 75.2 & 72.7 & 68.2 & 64.5 & 54.9 & 74.5 & 72.0 & 69.6 & 60.9 & 50.4 & 39.5 \\
            & JoAPR & 64.2 & 61.5 & 57.9 & 53.4 & 46.8 & 38.2 & 63.5 & 60.2 & 55.1 & 49.8 & 42.1 & 31.5 \\
            & NLPrompt & 73.6 & 74.2 & 70.5 & 68.5 & 65.7 & 59.6 & 73.4 & 72.5 & 70.5 & 63.5 & 57.8 & 47.4 \\
            & \textbf{OT-AdaGCE} & 73&72&70.1&69.2&64.3&59.9&73.1&71.2&70.4&64.6&60.1&47 \\
            \hline
            \textbf{Caltech101}
            & CoOp & 81.5 & 78.4 & 72.3 & 61.7 & 52.2 & 40.6 & 81.5 & 72.6 & 63.7 & 47.5 & 33.1 & 19.3 \\
            & GCE & 89.5 & 89.6 & 88.7 & 85.4 & 81.9 & 79.2 & 89.4 & 88.2 & 85.6 & 79.9 & 69.1 & 63.9 \\
            & JoAPR & 79.8 & 76.4 & 72.1 & 68.5 & 60.2 & 51.4 & 78.5 & 74.1 & 68.9 & 62.4 & 54.7 & 42.1 \\
            & NLPrompt & 92.2 & 91.3 & 89.9 & 89.7 & 88.6 & 85.6 & 91.8 & 91.6 & 90.5 & 89.3 & 85.6 & 76.1 \\
            & \textbf{OT-AdaGCE} & 88.6&87.8&87.2&85.1&81.3&80.8&89.4&87.4&86.7&83.9&80.3&71.4 \\
            \bottomrule
        \end{tabular}
    }
\end{table}

\subsection{Evaluation on Real-World Noise}
We further evaluated OT-AdaGCE on the Food101N dataset to test its robustness against real-world label noise.

As shown in Table~\ref{tab:innovation_food101n}, our method achieves an accuracy of \textbf{70.9\%}. While this outperforms the CoOp baseline (66.9\%), it is slightly lower than the reproduced NLPrompt (72.9\%). This result aligns with our observation on Caltech101, suggesting that without hyperparameter tuning ($k=1$), the soft-weighting strategy might be too conservative for certain real-world noise distributions compared to the hard-partitioning strategy.

\begin{table}[H]
    \centering
    \caption{\textbf{Real-world Noise Evaluation (Innovation)}}
    \label{tab:innovation_food101n}
    \begin{tabular}{lccccc}
        \toprule
        Method & CoOp & GCE & JoAPR & NLPrompt & \textbf{OT-AdaGCE} \\ 
        \midrule
        Accuracy & 66.9\% & 72.7\% & 72.5\% & \textbf{72.9\%} & 70.9\% \\ 
        \bottomrule
    \end{tabular}
\end{table}

\section{Conclusion}
In this project, we successfully reproduced \textit{NLPrompt}, validating its effectiveness against noisy labels in VLMs. From a convex optimization perspective, we rigorously analyzed the OT formulation via Bregman Projections and derived the appropriate method to handle class imbalance. Finally, to overcome the information loss in "hard partitioning", we proposed \textbf{OT-Guided Adaptive GCE}, a soft-weighting strategy that dynamically adjusts loss robustness based on optimal transport confidence.

\begin{ack}
    This project was completed for the SI251 Convex Optimization course at ShanghaiTech University, Fall 2025, under the instruction of Prof. Ye Shi.
\end{ack}

\small
\begin{thebibliography}{99}

    \bibitem[Pan et al.(2024)]{pan2024nlprompt}
    Bikang Pan, Qun Li, Xiaoying Tang, et al.
    \newblock NLPrompt: Noise-Label Prompt Learning for Vision-Language Models.
    \newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 2024.

    \bibitem[Zhang and Sabuncu(2018)]{zhang2018gce}
    Zhilu Zhang and Mert Sabuncu.
    \newblock Generalized Cross Entropy Loss for Training Deep Neural Networks with Noisy Labels.
    \newblock In {\em Advances in Neural Information Processing Systems (NeurIPS)}, 2018.

    \bibitem[Zhou et al.(2022)]{zhou2022coop}
    Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu.
    \newblock Learning to Prompt for Vision-Language Models.
    \newblock {\em International Journal of Computer Vision (IJCV)}, 2022.

    \bibitem[Radford et al.(2021)]{radford2021clip}
    Alec Radford, et al.
    \newblock Learning Transferable Visual Models From Natural Language Supervision.
    \newblock In {\em International Conference on Machine Learning (ICML)}, 2021.

    \bibitem[Khattak et al.(2023)]{khattak2023promptsrc}
    Muhammad Uzair Khattak, Hanoona Rasheed, Muhammad Maaz, Salman Khan, and Fahad Shahbaz Khan.
    \newblock Self-regulating Prompts: Foundational Model Adaptation without Forgetting.
    \newblock In {\em Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)}, 2023.

    \bibitem[Khattak et al.(2023)]{khattak2023maple}
    Muhammad Uzair Khattak, Hanoona Rasheed, Muhammad Maaz, Salman Khan, and Fahad Shahbaz Khan.
    \newblock MaPLe: Multi-modal Prompt Learning.
    \newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 2023.

    \bibitem[Jia et al.(2022)]{jia2022vpt}
    Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath Hariharan, and Ser-Nam Lim.
    \newblock Visual Prompt Tuning.
    \newblock In {\em Proceedings of the European Conference on Computer Vision (ECCV)}, 2022.

\end{thebibliography}

\end{document}
